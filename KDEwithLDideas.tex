\documentclass[letterpaper]{amsart}
\usepackage{mathtools,upref,siunitx,upquote,fancyvrb,xspace,color}
\usepackage[hyphens]{url}
\usepackage[utf8]{inputenc}
\usepackage{esdiff}
\usepackage{bbm}
\usepackage[capitalise]{cleveref}
\usepackage[margin=2.5cm]{geometry}
\input{FJHDef.tex}

\newtheorem{theorem}{Theorem}

\usepackage{algpseudocode}
\usepackage{algorithm, algorithmicx}
\algnewcommand\algorithmicparam{\textbf{Parameters:}}
\algnewcommand\PARAM{\item[\algorithmicparam]}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\RETURN{\State \textbf{Return }}

\newcommand{\hvarrho}{\widehat{\varrho}}
\newcommand{\KY}{K_y}
\newcommand{\tKY}{\widetilde{K}_y}
\newcommand{\KX}{{K_{\vx}}}
\newcommand{\hKX}{{\widehat{K}_{\vx}}}
\newcommand{\tk}{\tilde{k}}
\DeclareMathOperator{\ttk}{\widetilde{tail}}
\DeclareMathOperator{\smooth}{smth}
\DeclareMathOperator{\disc}{disc}

\allowdisplaybreaks[1]

\newcommand{\AGSNote}[1]{{\color{cyan} #1}}
\newcommand{\FJHNote}[1]{{\color{blue} #1}}

\begin{document}
\title{Kernel Density Estimation Using Low Discrepancy Sampling}
\author{Fred J. Hickernell}
\begin{abstract} Coming soon
\end{abstract}

\maketitle

\section{The Problem} \label{sec:problem}

Define the random variable $Y = f(\vX)$, where $\vX \sim \cu[0,1]^d$, and $f:[0,1]^d \to \reals$.  Low discrepancy sequences, $\{\vx_1, \vx_2, \ldots \}$, are used for accurately approximating  the population mean, $\mu = \Ex(Y)$, by the sample mean, $\hmu = n^{-1} \sum_{i=1}^n f(\vx_i)$.   Can low discrepancy (LD) sequences be used for estimating the density of $Y$ well?
%If $Y$ is a discrete or mixed random variable, can low discrepancy sequences also be used to estimate the locations and magnitudes of the jump discontinuities in $F$ as well as $F'$, where this derivative exists?

Suppose that  the random variable $Y$ is continuous, and has cumulative distribution function $F$ and probability density function $\varrho$.  Let $F_{\vy}$ be the empirical cumulative distribution function of a vector of sampled $Y$ values, $\vy := (y_1, \ldots, y_n) = (f(\vx_1), \ldots, f(\vx_n))$.  These values may be independent and identically distributed (IID) or sampled in some other way, such as via LD sampling.  Moreover, let $\mX$ denote the \emph{design}, i.e., the $n \times d$ matrix formed from the data sites, $\mX:=(\vx_1, \ldots, \vx_n)^T$, and let, let $F_\mX$ denote the empirical distribution of  these data sites.  Then a \emph{kernel density estimator (KDE)}, denoted $\hvarrho(\cdot,F_{\vy},k)$, may be defined  as
\begin{equation} \label{eq:kdedef}
	\hvarrho(y,F_{\vy},k) := \int_{-\infty}^{\infty} k(z,y) \, \dif F_{\vy}(z)  = \frac 1n \sum_{i=1}^n k(y_i,y) \\
	= \frac 1n \sum_{i=1}^n k(f(\vx_i),y) = \int_{[0,1]^d} k(f(\vx),y) \, \dif F_\mX(\vx),
\end{equation}
where $k:\reals \times \reals \to \reals$ is called a \emph{smoothing kernel}.  Here, $k(\cdot,y)$ is a weight function used to estimate the density at $y$, so $k(z,y)$ should typically be larger for $z$ near $y$ than for $z$ away from $y$.   Moreover, we assume that
\begin{equation}
	\label{eq:kintegral}
	\int_{-\infty}^{\infty} k(z,y) \, \dif y = 1 \qquad \forall z \in \reals.
\end{equation}
This implies that each datum, $y_i$, is given a weight of unity that is spread over the real line.  Consequently, $\int_{-\infty}^{\infty} \hvarrho(y,F_{\vy},k) \, dy = 1$.  If $k$ is non-negative, then the KDE, $\hvarrho(\cdot,F_{\vy},k)$, is non-negative, and so a true probability density.  But, we do not necessarily assume that $k$ is non-negative.


The error in approximating the density at a point by the KDE, $\abs{\varrho(y) - \hvarrho(y,F_{\vy},k)}$, can be bounded in terms of a sum of two terms:
\begin{align}
	\label{eq:KDEerrassum}
	\abs{\varrho(y) - \hvarrho(y,F_{\vy},k)} & =
	\abs{\varrho(y) - \hvarrho(y,F,k) + \hvarrho(y,F,k) - \hvarrho(y,F_{\vy},k)} \\
	\nonumber
	& \le  \abs{\varrho(y) - \hvarrho(y,F,k)} + \abs{\hvarrho(y,F,k) - \hvarrho(y,F_{\vy},k)},
\end{align}
where the smoothing kernel, $k$, defines a \emph{smoothed density}:
\begin{multline}
	\label{eq:smoothdens}
\hvarrho(y,F,k) := \int_{-\infty}^{\infty} k(z,y) \, \dif F(z)
= \int_{-\infty}^{\infty} k(z,y) \, \varrho(z) \, \dif z \\
= \int_{[0,1]^d} k(f(\vx),y) \,  \dif \vx
 = \int_{[0,1]^d} k(f(\vx),y) \,  \dif F_{\mathrm{unif}}(\vx),
\end{multline}
and $F_{\mathrm{unif}}$ denotes the uniform distribution function over the unit cube.  Then \eqref{eq:kdedef} and \eqref{eq:smoothdens} can be used to express the error bound in \eqref{eq:KDEerrassum} as
\begin{equation}
	\label{eq:KDEerrassumA}
	\abs{\varrho(y) - \hvarrho(y,F_{\vy},k)} \le \abs{\varrho(y) - \int_{-\infty}^{\infty} k(z,y) \, \varrho(z) \, \dif z } + \abs{\int_{[0,1]^d} k(f(\vx),y) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)},
\end{equation}
The first term on the right above is the error in approximating the true density by the smoothed density.  It can be made smaller by choosing $k(\cdot,y)$ to approximate the Dirac delta generalized function $\delta(\cdot - y)$.  The second term on the right in  \eqref{eq:KDEerrassum} is the error in approximating the integral defining the smoothed density by a cubature defined in terms of the design, $\mX$.  Choosing a peaky $k(\cdot,y)$  will tend to make this term large.  The error analysis in the following sections explains how to optimize the choice of the smoothing kernel, $k$, in light of this trade-off.



\section{Error Analysis}\label{sec:error_analysis}
Bounding the error of the KDE in terms of the sum of two terms facilitates the error analysis for the following reason:
\begin{itemize}
	\item The first term can be bounded in terms of the choice of smoothing kernel, $k$, by assuming that $\varrho$ lies in a Hilbert space with reproducing kernel $\KY:\reals \times \reals \to \reals$.

	\item The second term can be bounded in terms of the choice of smoothing kernel and design, by assuming that $k(f(\cdot),y)$  lies in a Hilbert space with reproducing kernel $\KX: [0,1]^d \times [0,1]^d\to \reals$ for all $y \in \reals$.
\end{itemize}

Under the first of these bulleted assumptions, an upper bound on the first term on the right in \eqref{eq:KDEerrassumA} can be bounded via the Cauchy-Schwarz inequality and the Riesz representation theorem:
\begin{align}
\label{eq:firstbd}
\abs{\varrho(y) - \int_{-\infty}^{\infty} k(z,y) \, \varrho(z) \, \dif z}
& = \abs{\ip[\KY]{\KY(\cdot,y) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \,  \dif z}{\varrho} } \\
\nonumber
& \le  \underbrace{\norm[\KY]{\KY(\cdot,y) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z}}_{=: \smooth(k,\KY)} \, \norm[\KY]{\varrho},
\end{align}
where the part of the error bound depending on the density kernel can be rewritten in terms of integrals:
\begin{align}
\label{eq:kquality}
\smooth^2(k,\KY) &= \norm[\KY]{\KY(\cdot,y) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z}^2 \\
\nonumber
&=  \KY(y,y) - 2 \int_{-\infty}^{\infty} k(z,y) \, \KY(z,y) \, \dif z  + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  k(z,y) \, \KY(z,t) \, k(t,y)\, \dif z \, \dif t.
\end{align}
With additional assumptions about the form of the smoothing kernel, it is possible to analyze how quickly this term vanishes as the smoothing kernel becomes more peaked.

Under the second bulleted assumption above, the second term on the right in \eqref{eq:KDEerrassumA} can also be bounded:
\begin{align}
	\label{eq:secondbd}
\abs{\int_{[0,1]^d} k(f(\vx),y) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)}
& = \abs{\ip[\KY]{\int_{[0,1]^d} \KX(\cdot,\vx) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)}{k(f(\cdot),y) }} \\
\nonumber
& \le  \underbrace{\norm[\KX]{\int_{[0,1]^d} \KX(\cdot,\vx) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)}}_{=:\disc(\mX,\KX)} \, \norm[\KX]{k(f(\cdot),y) } ,
\end{align}
where
\begin{align*}
\disc^2(\mX,\KX) & = \norm[\KX]{\int_{[0,1]^d} \KX(\cdot,\vx) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)}^2 \\
\nonumber
&=  \int_{[0,1]^d \times [0,1]^d} \KX(\vx,\vt) \, \dif \vx \, \dif \vt -
\frac 2n \sum_{i=1}^n \int_{[0,1]^d} \KX(\vx,\vx_i) \, \dif \vx  + \frac 1{n^2} \sum_{i,j=1}^n  \KX(\vx_i,\vx_j) .
\end{align*}
Here, the discrepancy, $\disc(\mX,\KX)$ measures the quality of the design, $\mX$, and $\norm[\KX]{k(f(\cdot),y) }$ measures of the peakiness or roughness of the smoothing kernel composed with $f$, the function defining the random variable of interest.

To summarize the argument above we have the following theorem.
\begin{theorem}\label{thm:errbd} The error of the KDE is bounded as
	\begin{equation}
			\abs{\varrho(y) - \hvarrho(y,F_{\vy},k)} \le \smooth(k,\KY) \norm[\KY]{\varrho} + \disc(\mX,\KX) \norm[\KX]{k(f(\cdot),y)}.
	\end{equation}
\end{theorem}
A KDE for general $f$ and corresponding $\varrho$, should
\begin{itemize}
	\item Choose $k$ to make $\smooth(k,\KY)$ small enough without making $\norm[\KX]{k(f(\cdot),y)}$ too large, and
	\item Choose $\mX$ to make the discrepancy small.
\end{itemize}


%% STOPPED HERE

\section{Stationary $k$ and $\KY$}

Returning to the error analysis of the kernel density estimator, we first consider the simpler case of stationary density kernels.  Suppose that $k(z,y) = h^{-1}\tk((z-y)/h)$ where $h$ is a bandwidth. We also assume for simplicity that $\tk$ is an even function.

Morever, assume that the reproducing kernel $\KY$ is also stationary, i.e., $\KY(y,z) = \tKY(y-z)$. This means that the density of the random variable $Y$ and the density of $Y$ plus any constant have the same norm in the Hilbert space defined by $\KY$.  By the definition of a reproducing kernel, $\tKY$ is also even. Then the quantity measuring the quality of the density kernel in \eqref{eq:kquality} may be written in terms of integrals involving the reproducing kernel and density kernel:
\begin{align*}
	\MoveEqLeft{\norm[\KY]{\KY(\cdot,y) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z}^2} \\
	&=  \tKY(0) -  \frac 2h \int_{-\infty}^{\infty} \tk((z-y)/h) \, \tKY(z-y) \, \dif z \\
	& \qquad \qquad + \frac 1{h^2} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \tk((z-y)/h)  \, \tKY(z-t) \, \tk((t-y)/h) \, \dif z \, \dif t \\
	&=   \tKY(0) - 2 \int_{-\infty}^{\infty} \tk(w) \, \tKY(hw) \, \dif w \\
	& \qquad \qquad + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \tk(w) \, \tKY(h(w-v)) \, \tk(v)\, \dif w \, \dif v,
\end{align*}
via the variable transformations $w = (z-y)/h$ and $v = (t-y)/h$. Note that the quality of the density kernel is independent of the position, $y$ for stationary kernels.

To understand the dependence of this quantity on the bandwidth, $h$, let's approximate $\tKY$ by a MacLaurin series with $r+1$ terms plus an error term.  Since $\tKY$ is an even function, the odd numbered derivatives drop out.
\begin{equation*}
	\tKY(y)  = \sum_{\ell = 0}^{r} \frac{\tKY^{(2\ell)}(0) \, y^{2\ell}}{2\ell!} + \int_0^y \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!} \, \dif z.
\end{equation*}
Here $\tKY^{(2\ell)}$ denotes the $2\ell^{\text{th}}$ derivative of $\tKY$.

It follows from this series expression for $\tKY$ that
\begin{align} \label{eq:isomidterm}
	\int_{-\infty}^{\infty} \tk(w) \, \tKY(hw) \, \dif w & =
	\int_{-\infty}^{\infty} \tk(w) \, \sum_{\ell = 0}^{r} \frac{\tKY^{(2\ell)}(0) \, (hw)^{2\ell}}{2\ell!}  \, \dif w \\
	&
	\nonumber
	\qquad \qquad
	+ 2 \int_{0}^{\infty} \tk(w) \,  \int_0^{hw} \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!} \, \dif z  \, \dif w.
\end{align}
If $\tk$ has the property that
\begin{equation} \label{eq:tkprop}
	\int_{-\infty}^{\infty} \tk(w) w^{2\ell}  \, \dif w =  \delta_{0,\ell}, \qquad l=0,\dots,r,
\end{equation}
then the first term on the right side of \eqref{eq:isomidterm} reduces to $\tKY(0)$, recalling that $\int_{-\infty}^\infty \tilde{k}(w) \dif w = 1$ from \eqref{eq:kintegral}.   By reordering the iterated integral, the  second term on the right side of  \eqref{eq:isomidterm} can be written as
\begin{align*}
	\MoveEqLeft{2 \int_{0}^{\infty}  \int_{z/h}^\infty \tk(w) \, \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!}   \, \dif w \, \dif z} \\
	& =  2\int_{0}^{\infty}  \ttk(z/h) \, \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!}    \, \dif z,
	\quad \ttk(z) := \int_{z}^\infty \tk(w) \, \dif w \\
	& =  \frac{2h^{2r+3}}{(2r+2)!} \int_{0}^{\infty}  \ttk(v) \, \tKY^{(2r+2)}(hv) \,  v^{2r+2}    \, \dif v \\
	&  \le  \frac{2h^{2r+3}}{(2r+2)!}  \norm[\infty]{\tKY^{(2r+2)}} \int_{0}^{\infty}  \abs{\ttk(v)}  \,  v^{2r+2}    \, \dif v \\
\end{align*}
using the substitution $v = z/h$.

\section{Hermite Function Density Kernels}

Let $H_m$ denote the (``physicist's) Hermite polynomials, written as
\begin{equation*}
	H_m(y) = (-1)^m \exp(y^2) \, \frac{\dif^m \exp(-y^2)}{\dif y^m}, \; m \in \natzero, \qquad
	H_0(y) = 1, \ H_2(y) = 4y^2-2, \ H_4(y) = 16 y^4 - 48 y^2 + 12, \ \ldots.
\end{equation*}
These polynomials are orthogonal with respect to the weight $y \mapsto \exp(-y^2)$, i.e.,
\begin{equation*}
	\int_{-\infty}^{\infty} H_m(y) H_{m'}(y) \exp(-y^2) = \sqrt{\pi} 2^m m! \delta_{m,m'}, \quad m,m' \in \natzero.
\end{equation*}
Taking $\tk$ to be a linear combination of the first $r$ even Hermite polynomials multiplied by the weight function and enforcing \eqref{eq:tkprop} yields
\begin{align*}
	r& = 0  & \tk(y) &= \frac{\exp(-y^2)}{\sqrt{\pi}}
		\\
	r& = 1 &  \tk(y) & =\frac{\left(3 - 2y^{2}\right) \exp(- y^{2})}{2\sqrt{\pi}} \\
	r &= 2 & \tk(y) & = \frac{\left(4 y^{4} - 20 y^{2} + 15\right) \exp(- y^{2})}{8 \sqrt{\pi}}
\end{align*}

\end{document}

\section{Error in Estimating the Smoothed Density with Tensor Product Kernels}\label{sec:errsmthdens}
Now we look at the second part of the error namely, $	\abs{\hvarrho(y,\varrho,k) - \hvarrho(y,\varrho_{\vy},k)}$.  We also assume that $\KX$ is of tensor product form, namely,
\begin{equation*}
	\KX(\vt,\vx) = \prod_{\ell = 1}^d [1 + \gamma_\ell \hKX(t_\ell,x_\ell) ] = \sum_{\fu \subseteq \{1, \ldots, d\}} \gamma_{\fu} \KX_{\fu}(\vt_\fu,\vx_\fu),
\end{equation*}
where the $\gamma_\ell$ are coordinate weights, $\gamma_\fu = \prod_{\ell \in \fu} \gamma_\ell}$, and $\KX_{\fu}(\vt_\fu,\vx_\fu) = \prod_{\ell \in \fu} \hKX(t_\ell,x_\ell)$.
Moreover, $\vx_\fu$ is the vector of components of $\vx$ indexed by the set $\fu$. Any $g$ in the Hilbert space with reproducing kernel $\KX$, one may be decomposed as $g(\vx) = \sum_{\fu \subseteq \{1, \ldots, d\}} g_\fu(\vx_\fu)$, where each $g_\fu$ is in the Hilbert space with reproducing kernel $\hKX_\fu$.

Under these assumptions, the error in \eqref{eq"??} can be bounded as
\begin{equation*}
	\abs{\hvarrho(y,\varrho,k) - \hvarrho(y,\varrho_{\vy},k)} \le  \norm[\KX]{\int_{[0,1]^d} \KX(\cdot,\vx) \, \dif \vx -
		\frac 1n \sum_{i=1}^n \KX(\cdot,\vx_i)} \norm[\KX]{k(f(\cdot),y) } ,
\end{equation*}
where
\begin{align*}
	\MoveEqLeft{\norm[\KX]{\int_{[0,1]^d} \KX(\cdot,\vx) \, \dif \vx -
			\frac 1n \sum_{i=1}^n \KX(\cdot,\vx_i)}^2} \\
	&=  \sum_{\fu \subseteq \{1, \ldots, d\}} \gamma_\fu \bigg [\int_{[0,1]^\fu \times [0,1]^\fu} \KX_\fu(\vx_\fu,\vt_\fu) \, \dif \vx_\fu \, \dif \vt_\fu -
	\frac 2n \sum_{i=1}^n \int_{[0,1]^\fu} \KX(\vx_\fu,\vx_{i,\fu}) \, \dif \vx_\fu \\
	& \qquad \qquad + \frac 1{n^2} \sum_{i,j=1}^n  \KX(\vx_{i,\fu},\vx_{j,\fu}) \right \bigg].
\end{align*}
This is an upper bound on the second term in the expression for the error that separates the part depending on the choice of the density kernel evaluated at $(f(\cdot),y)$ from the part depending on the sample nodes.










\bibliographystyle{alphadin}
\bibliography{FJH23,FJHown23}

\end{document}



\section{Stationary Density Kernels}

Suppose that $k(z,y) = h^{-1}\tk((z-y)/h)$ where $h$ is a bandwidth, and $\KY(y,z) = \tKY(y-z)$.  Then the quantity measuring the quality of the density kernel in \eqref{eq:kquality} may be written as
\begin{align*}
	\MoveEqLeft{\norm[\KY]{\KY(\cdot,y) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z}^2} \\
	&=  \tKY(0) -  \frac 2h \int_{-\infty}^{\infty} \tk((z-y)/h) \, \tKY(z-y) \, \dif z \\
	& \qquad \qquad + \frac 1{h^2} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \tk((z-y)/h)  \, \tKY(z-t) \, \tk((t-y)/h) \, \dif z \, \dif t \\
	&=   \tKY(0) - \frac{2}h \int_{-\infty}^{\infty} \tk(w/h) \, \tKY(w) \, \dif w \\
	& \qquad \qquad + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \tk(w) \, \tKY(h(w-v)) \, \tk(v)\, \dif w \, \dif v
	.
\end{align*}
via the variable transformations $w = (z-y)$ and $v = (t-y)$.

Let's approximate $\tKY$ by a MacLaurin series with $r+1$ terms.  Since $\tKY$ is an even function, the odd numbered derivatives drop out.
\begin{equation*}
	\tKY(y)  = \sum_{\ell = 0}^{r} \frac{\tKY^{(2\ell)}(0) \, y^{2\ell}}{2\ell!} + \int_0^y \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!} \, \dif z.
\end{equation*}
Here $\tKY^{(2\ell)}$ denotes the $2\ell^{\text{th}}$ derivative of $\tKY$.

It follows from this approximation that
\begin{align} \label{eq:isomidterm}
	\frac 1h \int_{-\infty}^{\infty} \tk(w/h) \, \tKY(w) \, \dif w & =
	\frac 1h \int_{-\infty}^{\infty} \tk(w/h) \, \sum_{\ell = 0}^{r} \frac{\tKY^{(2\ell)}(0) \, w^{2\ell}}{2\ell!}  \, \dif w \\
	&
	\nonumber
	\qquad \qquad
	+ \frac 1h  \int_{-\infty}^{\infty} \tk(w/h) \,  \int_0^{w} \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!} \, \dif z  \, \dif w
\end{align}
If $\tk$ has the property that
\begin{equation*}
	\int_{-\infty}^{\infty} \tk(w) w^{2\ell}  \, \dif w =  \delta_{0,\ell},
\end{equation*}
then the first term on the right side of \eqref{eq:isomidterm} reduces to $\tKY(0)$.  By reordering the iterated integral, the  second term on the right side of  \eqref{eq:isomidterm} can be written as
\begin{align*}
	\int_{0}^{\infty}  \frac 1h  \int_{z}^\infty \tk(w/h) \, \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!}   \, \dif w \, \dif z
	& =  \int_{0}^{\infty}  \ttk(z/h) \, \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!}    \, \dif z
\end{align*}



