\documentclass[letterpaper]{amsart}
\usepackage{mathtools,upref,siunitx,upquote,fancyvrb,xspace,color}
\usepackage[hyphens]{url}
\usepackage[utf8]{inputenc}
\usepackage{esdiff}
\usepackage{bbm}
\usepackage[capitalise]{cleveref}
\usepackage[margin=2.5cm]{geometry}
\usepackage[colorinlistoftodos]{todonotes}
% \usepackage{marginpar}
\input{FJHDef.tex}

\newtheorem{theorem}{Theorem}

\usepackage{algpseudocode}
\usepackage{algorithm, algorithmicx}
\algnewcommand\algorithmicparam{\textbf{Parameters:}}
\algnewcommand\PARAM{\item[\algorithmicparam]}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\RETURN{\State \textbf{Return }}

\newcommand{\hvarrho}{\widehat{\varrho}}
\newcommand{\KY}{K_y}
\newcommand{\tKY}{\widetilde{K}_y}
\newcommand{\KX}{{K_{\vx}}}
\newcommand{\KXu}{K_{\vx,\fu}}
\newcommand{\hKX}{{\widehat{K}_{\vx}}}
\newcommand{\tk}{\tilde{k}}
\newcommand{\fv}{\mathfrak{v}}
\DeclareMathOperator{\ttk}{\widetilde{tail}}
\DeclareMathOperator{\smooth}{smth}
\DeclareMathOperator{\disc}{disc}

\allowdisplaybreaks[1]

\newcommand{\AGSNote}[1]{{\color{cyan} #1}}
\newcommand{\FJHNote}[1]{{\color{blue} #1}}
\newcommand{\AiwenNote}[1]{{\color{green} #1}}
\newcounter{probcnt}
\setcounter{probcnt}{0}
\newenvironment{Problem}{\bigskip \LARGE \color{red} \refstepcounter{probcnt} \noindent Problem \theprobcnt. }{\normalsize \color{black} \bigskip}

\begin{document}
\title{Kernel Density Estimation Using Low Discrepancy Sampling}
\author{Fred J. Hickernell}
\author{Guillem}
\author{Aiwen Li}
\author{Luke}
\author{Brooke Feinberg}
\author{Richard Varela}
\author{Aadit Jain}
\begin{abstract} Coming soon
\end{abstract}

\maketitle

\section{The Problem} \label{sec:problem}

Define the random variable $Y = f(\vX)$, where $\vX \sim \cu[0,1]^d$, and $f:[0,1]^d \to \reals$.  Low discrepancy sequences, $\{\vx_1, \vx_2, \ldots \}$, are used for accurately approximating  the population mean, $\mu = \Ex(Y)$, by the sample mean, $\hmu = n^{-1} \sum_{i=1}^n f(\vx_i)$.   Can low discrepancy (LD) sequences be used for estimating the density of $Y$ well?

See \cite{DicEtal14a} for a tutorial on methods of low discrepancy sequences.
%If $Y$ is a discrete or mixed random variable, can low discrepancy sequences also be used to estimate the locations and magnitudes of the jump discontinuities in $F$ as well as $F'$, where this derivative exists?

Suppose that  $Y$ is a continuous random variable $Y$ with cumulative distribution function $F$ and probability density function $\varrho$.  Let $F_{\vy}$ be the empirical cumulative distribution function of a vector of sampled $Y$ values, $\vy := (y_1, \ldots, y_n) = (f(\vx_1), \ldots, f(\vx_n))$.  These values may be independent and identically distributed (IID) or sampled in some other way, such as via LD sampling.  Moreover, let $\mX$ denote the \emph{design}, i.e., the $n \times d$ matrix formed from the data sites, $\mX:=(\vx_1, \ldots, \vx_n)^T$, and let, let $F_\mX$ denote the empirical distribution of  these data sites.  Then a \emph{kernel density estimator (KDE)}, denoted $\hvarrho(\cdot,F_{\vy},k)$, may be defined  in terms of a Riemann-Stieltjes integral, or equivalently, a sum:
\begin{equation} \label{eq:kdedef}
	\hvarrho(y,F_{\vy},k) := \int_{-\infty}^{\infty} k(z,y) \, \dif F_{\vy}(z)  = \frac 1n \sum_{i=1}^n k(y_i,y) \\
	= \frac 1n \sum_{i=1}^n k(f(\vx_i),y) = \int_{[0,1]^d} k(f(\vx),y) \, \dif F_\mX(\vx),
\end{equation}
where $k:\reals \times \reals \to \reals$ is called a \emph{smoothing kernel}.  Here, $k(\cdot,y)$ is a weight function used to estimate the density at $y$.  Thus, we expect $k(z,y)$ to be typically larger for $z$ near $y$ than for $z$ away from $y$.   Moreover, we assume that
\begin{equation}
	\label{eq:kintegral}
	\int_{-\infty}^{\infty} k(z,y) \, \dif y = 1 \qquad \forall z \in \reals.
\end{equation}
This implies that each datum, $y_i$, is given a weight of unity that is spread over the real line.  Consequently, $\int_{-\infty}^{\infty} \hvarrho(y,F_{\vy},k) \, dy = 1$.  If $k$ is non-negative, then the KDE, $\hvarrho(\cdot,F_{\vy},k)$, is non-negative, and so a true probability density.  But, we do not necessarily assume that $k$ is non-negative.

If the design, $\mX$, is chosen to be IID, in which case, $\vy$ is a vector of IID elements, then the expected KDE is a \emph{smoothed density}, i.e., $\Ex[\hvarrho(y,F_{\vy},k)] = \hvarrho(y,F,k)$, where
\begin{multline}
	\label{eq:smoothdens}
	\hvarrho(y,F,k) := \int_{-\infty}^{\infty} k(z,y) \, \dif F(z)
	= \int_{-\infty}^{\infty} k(z,y) \, \varrho(z) \, \dif z \\
	= \int_{[0,1]^d} k(f(\vx),y) \,  \dif \vx
	= \int_{[0,1]^d} k(f(\vx),y) \,  \dif F_{\mathrm{unif}}(\vx),
\end{multline}
where $F_{\mathrm{unif}}$ denotes the uniform distribution function over the unit cube.
For designs $\mX$ chosen to be LD one expects $\hvarrho(y,F_{\vy},k)$ to tend to $\hvarrho(y,F,k)$ as $n \to \infty$.

The error in approximating the density at a point by the KDE, $\abs{\varrho(y) - \hvarrho(y,F_{\vy},k)}$, can be bounded in terms of a sum of two terms involving the smoothed density:
\begin{align}
	\label{eq:KDEerrassum}
	\abs{\varrho(y) - \hvarrho(y,F_{\vy},k)} & =
	\abs{\varrho(y) - \hvarrho(y,F,k) + \hvarrho(y,F,k) - \hvarrho(y,F_{\vy},k)} \\
	\nonumber
	& \le  \abs{\varrho(y) - \hvarrho(y,F,k)} + \abs{\hvarrho(y,F,k) - \hvarrho(y,F_{\vy},k)},
\end{align}
where $F_{\mathrm{unif}}$ denotes the uniform distribution function over the unit cube.  Then \eqref{eq:kdedef} and \eqref{eq:smoothdens} can be used to express the error bound in \eqref{eq:KDEerrassum} as
\begin{equation}
	\label{eq:KDEerrassumA}
	\abs{\varrho(y) - \hvarrho(y,F_{\vy},k)} \le \abs{\varrho(y) - \int_{-\infty}^{\infty} k(z,y) \, \varrho(z) \, \dif z } + \abs{\int_{[0,1]^d} k(f(\vx),y) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)},
\end{equation}
The first term on the right above is the error in approximating the true density by the smoothed density.  It can be made smaller by choosing $k(\cdot,y)$ to approximate the Dirac delta generalized function $\delta(\cdot - y)$.  The second term on the right in  \eqref{eq:KDEerrassum} is the error in approximating the integral defining the smoothed density by a cubature defined in terms of the design, $\mX$.  Choosing a peaky $k(\cdot,y)$  will tend to make this term large.  The error analysis in the following sections explains how to optimize the choice of the smoothing kernel, $k$, in light of this trade-off.

Related work on QMC for density estimation has been done by \cite{AbdEtal21a,GilKuoSlo23a,LEcPuc20a,LEcuyer2022b}.

\section{Error Analysis via Reproducing Kernel Hilbert Spaces}\label{sec:error_analysis}
Bounding the error of the KDE in terms of the sum of two terms facilitates the error analysis for the following reason:
\begin{itemize}
	\item The first term can be bounded in terms of the choice of smoothing kernel, $k$, by assuming that $\varrho$ lies in a Hilbert space with reproducing kernel $\KY:\reals \times \reals \to \reals$.

	\item The second term can be bounded in terms of the choice of smoothing kernel and design, by assuming that $k(f(\cdot),y)$  lies in a Hilbert space with reproducing kernel $\KX: [0,1]^d \times [0,1]^d\to \reals$ for all $y \in \reals$.
\end{itemize}

Under the first of these bulleted assumptions, an upper bound on the first term on the right in \eqref{eq:KDEerrassumA} can be bounded via the Cauchy-Schwarz inequality and the Riesz representation theorem:
\begin{align}
\label{eq:firstbd}
\abs{\varrho(y) - \int_{-\infty}^{\infty} k(z,y) \, \varrho(z) \, \dif z}
& = \abs{\ip[\KY]{\KY(\cdot,y) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \,  \dif z}{\varrho} } \\
\nonumber
& \le  \underbrace{\norm[\KY]{\KY(\cdot,y) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z}}_{=: \smooth(k,\KY)} \, \norm[\KY]{\varrho},
\end{align}
where the part of the error bound depending on the density kernel can be rewritten in terms of integrals:
\begin{align}
\label{eq:kquality}
\smooth^2(k,\KY) &= \norm[\KY]{\KY(\cdot,y) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z}^2 \\
\nonumber
&=  \KY(y,y) - 2 \int_{-\infty}^{\infty} k(z,y) \, \KY(z,y) \, \dif z  + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  k(z,y) \, \KY(z,t) \, k(t,y)\, \dif z \, \dif t.
\end{align}
See \cref{app:rkhs} for an explanation of this derivation.  With additional assumptions about the form of the smoothing kernel, it is possible to analyze how quickly this term vanishes as the smoothing kernel becomes more peaked.

Under the second bulleted assumption above, the second term on the right in \eqref{eq:KDEerrassumA} can also be bounded:
\begin{align}
	\label{eq:secondbd}
\abs{\int_{[0,1]^d} k(f(\vx),y) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)}
& = \abs{\ip[\KY]{\int_{[0,1]^d} \KX(\cdot,\vx) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)}{k(f(\cdot),y) }} \\
\nonumber
& \le  \underbrace{\norm[\KX]{\int_{[0,1]^d} \KX(\cdot,\vx) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)}}_{=:\disc(\mX,\KX)} \, \norm[\KX]{k(f(\cdot),y) } ,
\end{align}
where
\begin{align*}
\disc^2(\mX,\KX) & = \norm[\KX]{\int_{[0,1]^d} \KX(\cdot,\vx) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)}^2 \\
\nonumber
&=  \int_{[0,1]^d \times [0,1]^d} \KX(\vx,\vt) \, \dif \vx \, \dif \vt -
\frac 2n \sum_{i=1}^n \int_{[0,1]^d} \KX(\vx,\vx_i) \, \dif \vx  + \frac 1{n^2} \sum_{i,j=1}^n  \KX(\vx_i,\vx_j) .
\end{align*}
Here, the discrepancy, $\disc(\mX,\KX)$ measures the quality of the design, $\mX$, and $\norm[\KX]{k(f(\cdot),y) }$ measures of the peakiness or roughness of the smoothing kernel composed with $f$, the function defining the random variable of interest.

The argument above is summarized in the theorem below.  This error bound highlights the quantities that we should optimize to make the error as small as possible.
\begin{theorem}\label{thm:errbd} The error of the KDE is bounded as
	\begin{equation}
			\abs{\varrho(y) - \hvarrho(y,F_{\vy},k)} \le \smooth(k,\KY) \norm[\KY]{\varrho} + \disc(\mX,\KX) \norm[\KX]{k(f(\cdot),y)}.
	\end{equation}
\end{theorem}
A good KDE should apply to a wide range of  functions, $f$, which defined $Y$ and a corresponding wide range of probability densities, $\varrho$.  Therefore, our choices should involve
\begin{itemize}
	\item  Smoothing kernel $k$, which makes $\smooth(k,\KY)$ small enough without making $\norm[\KX]{k(f(\cdot),y)}$ too large, and
	\item Design $\mX$, which makes the discrepancy small.
\end{itemize}
These choices will depend on the assumptions inherent in our choices of reproducing kernels $\KY$ and $\KX$, but the error bound in \cref{thm:errbd} illuminates the effects of these choices.


%% STOPPED HERE

\section{Stationary $k$ and $\KY$}

Returning to the error analysis of the kernel density estimator, we first consider the simpler case of stationary density kernels.  Suppose that $k(z,y) = h^{-1}\tk((z-y)/h)$ where $h$ is a bandwidth. We also assume for simplicity that $\tk$ is an even function.

\subsection{Bounding the Smoothing} Moreover, assume that the reproducing kernel $\KY$ is also stationary, i.e., $\KY(y,z) = \tKY(y-z)$. This means that the density of the random variable $Y$ and the density of $Y$ plus any constant have the same norm in the Hilbert space defined by $\KY$.  By the definition of a reproducing kernel, $\tKY$ is also even. Then the quantity measuring the quality of the density kernel in \eqref{eq:kquality} may be written in terms of integrals involving the reproducing kernel and density kernel:
\begin{align*}
	\smooth^2(k,\KY) & = \norm[\KY]{\KY(\cdot,y) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z}^2 \\
	&=  \tKY(0) -  \frac 2h \int_{-\infty}^{\infty} \tk((z-y)/h) \, \tKY(z-y) \, \dif z \\
	& \qquad \qquad + \frac 1{h^2} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \tk((z-y)/h)  \, \tKY(z-t) \, \tk((t-y)/h) \, \dif z \, \dif t \\
	&=   \tKY(0) - 2 \int_{-\infty}^{\infty} \tk(w) \, \tKY(hw) \, \dif w \\
	& \qquad \qquad + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \tk(w) \, \tKY(h(w-v)) \, \tk(v)\, \dif w \, \dif v,
\end{align*}
via the variable transformations $w = (z-y)/h$ and $v = (t-y)/h$. Note that the quality of the density kernel is independent of the position, $y$ for stationary kernels.

To understand the dependence of this quantity on the bandwidth, $h$, let's approximate $\tKY$ by a MacLaurin series with $r+1$ terms plus an error term.  Since $\tKY$ is an even function, the odd numbered derivatives drop out.
\begin{equation*}
	\tKY(y)  = \sum_{\ell = 0}^{r} \frac{\tKY^{(2\ell)}(0) \, y^{2\ell}}{2\ell!} + \int_0^y \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!} \, \dif z.
\end{equation*}
Here $\tKY^{(2\ell)}$ denotes the $2\ell^{\text{th}}$ derivative of $\tKY$.

It follows from this series expression for $\tKY$ that
\begin{align} \label{eq:isomidterm}
	\int_{-\infty}^{\infty} \tk(w) \, \tKY(hw) \, \dif w & =
	\int_{-\infty}^{\infty} \tk(w) \, \sum_{\ell = 0}^{r} \frac{\tKY^{(2\ell)}(0) \, (hw)^{2\ell}}{2\ell!}  \, \dif w \\
	&
	\nonumber
	\qquad \qquad
	+ 2 \int_{0}^{\infty} \tk(w) \,  \int_0^{hw} \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!} \, \dif z  \, \dif w.
\end{align}
If $\tk$ has the property that
\begin{equation} \label{eq:tkprop}
	\int_{-\infty}^{\infty} \tk(w) w^{2\ell}  \, \dif w =  \delta_{0,\ell}, \qquad l=0,\dots,r,
\end{equation}
then the first term on the right side of \eqref{eq:isomidterm} reduces to $\tKY(0)$, recalling that $\int_{-\infty}^\infty \tilde{k}(w) \dif w = 1$ from \eqref{eq:kintegral}.   By reordering the iterated integral, the  second term on the right side of  \eqref{eq:isomidterm} can be written as
\begin{align*}
	\MoveEqLeft{2 \int_{0}^{\infty}  \int_{z/h}^\infty \tk(w) \, \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!}   \, \dif w \, \dif z} \\
	& =  2\int_{0}^{\infty}  \ttk(z/h) \, \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!}    \, \dif z,
	\quad \ttk(z) := \int_{z}^\infty \tk(w) \, \dif w \\
	& =  \frac{2h^{2r+3}}{(2r+2)!} \int_{0}^{\infty}  \ttk(v) \, \tKY^{(2r+2)}(hv) \,  v^{2r+2}    \, \dif v \\
	&  \le  \frac{2h^{2r+3}}{(2r+2)!}  \norm[\infty]{\tKY^{(2r+2)}} \int_{0}^{\infty}  \abs{\ttk(v)}  \,  v^{2r+2}    \, \dif v \\
\end{align*}
using the substitution $v = z/h$.  Thus,
\[
- 2 \int_{-\infty}^{\infty} \tk(w) \, \tKY(hw) \, \dif w  = -2\tKY(0) + \Order(h^{2r+3}).
\]

\begin{Problem} [Easier]
    Can we also show that
    \[
    \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \tk(w) \, \tKY(h(w-v)) \, \tk(v)\, \dif w \, \dif v = \tKY(0) + \Order(h^{2r+3}),
    \]
    which would mean that $\smooth^2(k,\KY) =\Order(h^{2r+3}) $?
\end{Problem}


\textbf{Attempted Solution (Luke Bielawski)}
\marginpar{Your comments welcome}
We want to show that
\[
\int_{-\infty}^\infty \int_{-\infty}^\infty \tk(w) \tKY(h(w-v)) \tk(v) \, \dif w \, \dif v = \tilde{K_y}(0) + \Order(h^{2r+3})
\]
Since $\tKY(x)$ is an even function, we can take its MacLaurin series of $r+1$ terms which are all even
\[
\tKY(x) = \sum_{l=0}^r \frac{1}{(2l)!}\tKY^{(2l)}(0) x^{2l} + \int_0^x \frac{1}{(2r+2)!}\tKY^{(2r+2)}(z) z^{2r+2}dz
\]
Thus, we have
\[
\tKY(h(w-v)) = \sum_{l=0}^r \frac{1}{(2l)!}\tKY^{(2l)}(0) h^{2l}(w-v)^{2l} + \int_0^{h(w-v)} \frac{1}{(2r+2)!}\tKY^{(2r+2)}(z) z^{2r+2} \, \dif z
\]
We expand the left term with the binomial theorem and use the substitution $q = \frac{1}{h}z$
\[
\tKY(h(w-v)) = \sum_{l=0}^r \frac{h^{2l}}{(2l)!}\tKY^{(2l)}(0) \sum_{j=0}^{2l}\binom{2l}{i}(-1)^iw^{2l-i}v^i + \int_0^{w-v} \frac{1}{(2r+2)!}\tKY^{(2r+2)}(q) h^{2r+3}q^{2r+2} \,\dif q
\]
We adopt the same condition as before \todo[inline]{had to change bound to $l = 0, ..., 2r$ because of error: didn't consider $i = 0, 2l$ in left binomial term}
\[
\int_{-\infty}^\infty \tk(w)w^l \, \dif w = \delta_{0, l} \qquad  l = 0, ..., 2r
\]
For $l = 0$, this is just requiring that $\tk$ is normalized to 1. For odd $l$, this is immediate (up to issues of convergence), since $\tk$ is even, meaning $\tk(w)w^l$ is odd. For even $l$, this is just a weaker version of the prior requirement. Putting the pieces together, we have
\begin{align*}
    \int_{-\infty}^\infty \int_{-\infty}^\infty \tk(w) \tKY(h(w-v)) \tk(v) \, \dif w \, \dif v &= \int_{-\infty}^\infty \int_{-\infty}^\infty \tk(w) \tk(v) \, \sum_{l=0}^r \frac{h^{2l}}{(2l)!}\tKY^{(2l)}(0) \sum_{j=0}^{2l}\binom{2l}{i}(-1)^iw^{2l-i}v^i \, \dif w \, \dif v \\
    &+ \int_{-\infty}^\infty \int_{-\infty}^\infty \tk(w) \tk(v) \,\left(\int_0^{w-v} \frac{1}{(2r+2)!}\tKY^{(2r+2)}(q) h^{2r+3}q^{2r+2} \,\dif q\right) \, \dif v \, \dif w
\end{align*}
Rearranging gives
\begin{align*}
    \sum_{l=0}^r h^{2l}\frac{1}{(2l)!}\tKY^{(2l)}(0) \sum_{i=0}^{2l}\binom{2l}{i}(-1)^i \int_{-\infty}^\infty  \tk(v) v^i \dif v\int_{-\infty}^\infty \tk(w) w^{2l-i} \dif w \\
    + h^{2r+3} \int_{-\infty}^\infty \int_{-\infty}^\infty \tk(w) \tk(v) \int_0^{w-v}
 \tKY^{(2r+2)}(q)q^{2r+2} \dif q \, \dif w \, \dif v
\end{align*}
For the first expression, for $i = 1, ..., 2r$, the $v$-integral vanishes. For $i = 0, ..., 2r-1$, the $w$-integral vanishes, unless $2l = 0$. Thus, only $i = l = 0$ term survives, which gives $\tKY(0)$. For the second expression, notice that everything to the right of $h^{2r+3}$ is a constant with respect to $h$. One can concoct a more precise constant; however, I don't think its vital. Thus, let the entire expression
\[
C = \int_{-\infty}^\infty \int_{-\infty}^\infty \tk(w) \tk(v) \int_0^{w-v}
 \tKY^{(2r+2)}(q)q^{2r+2} \dif q \, \dif w \, \dif v < \infty
\]
meaning we have
\[
    \int_{-\infty}^\infty \int_{-\infty}^\infty \tk(w) \tKY(h(w-v)) \tk(v) \, \dif w \, \dif v = \tilde{K_y}(0) + Ch^{2r+3} = \tilde{K_y}(0) + \Order(h^{2r+3})
\]
as desired.
\subsection{Bounding the Variation}
For certain reproducing kernels $\KX$, we can write down a formula for the norm, $\norm[\KX]{\cdot}$.  For example, for the weighted centered discrepancy, we have
\begin{equation*}
    \KX(\vt,\vx) = \prod_{j=1}^d \Bigl\{1 + \frac{\gamma_j^2}{2}[\abs{t_j -1/2} + \abs{x_j-1/2} - \abs{t_j  - x_j}] \Bigr\}
\end{equation*}
where $\{\gamma_j\}_{j=1}^d$ are coordinate weights \cite[Eq.\ (31)]{Hic99b}.  The norm related to this reproducing kernel is
\begin{align*}
    \norm[\KX]{g}^2 = \sum_{\emptyset \ne \fu \subseteq \{1, \ldots, d\}}
    \prod_{j \in \fu} \frac{1}{\gamma_j^2}
    \int_{[0,1]^{\Bar{\fu}}} \abs{ \left . \frac{\partial^{\abs{\fu}}g}{\partial \vx_{\fu}} \right \rvert_{\vx_{\fu} = (1/2, \ldots, 1/2)}}^2 \, \dif \vx_{\bar{\fu}}, \qquad \bar{\fu} := \{1, \ldots, d\} \setminus \fu
\end{align*}

See \cref{sec:decomp} for some thoughts that might help.


\begin{Problem} [Harder]
    We want to analyze how
    \[
    \norm[\KX]{k\bigl(f(\cdot),y\bigr)} = \norm[\KX]{\tk\bigl((y - f(\cdot))/h\bigr)/h}
    \]
    grows as $h$ decreases.  How does this depend on the coordinate weights?

    We may try following the argument in \cite[Proposition 4.1]{AbdEtal21a}, where no coordinate weights are used.  The coordinate weights may help overcome the curse of dimensionality.
\end{Problem}

\begin{Problem} [Before the previous problem, easier]
We know that the mean square error of integration using independent and identically distributed (IID) sampling is
\begin{equation*}
    \frac{\var\bigl(\tk((y - f(\cdot))/h)/h\bigr)}{n} =  \frac{\var\bigl(\tk((y - f(\cdot))/h)\bigr)}{h^2 n},
\end{equation*}
where the variance of a function defined on $[0,1]^d$ in this case is defined as
\begin{equation*}
\var(g) = \int_{[0,1]^d} [g(\vx)]^2 \, \dif \vx - \Biggl[ \int_{[0,1]^d} g(\vx) \, \dif \vx  \Biggr]^2.
\end{equation*}
See \cref{app:MSE} for a derivation.
Can we bound or analyze $ \var(\tk((y - f(\cdot))/h))/(h^2n)$ as $h \to 0$?  Taylor series might help.

Maybe an easier version is to consider
\begin{equation*}
    \frac{\var\bigl(\tk((y - Y)/h)/h\bigr)}{n} =  \frac{\var\bigl(\tk((y - Y)/h)\bigr)}{h^2 n},
\end{equation*}
where
\begin{align*}
\var\bigl(\tk((y - Y)/h)\bigr) &= \int_{-\infty}^{\infty} [\tk((y - z)/h)]^2\, \varrho(z) \, \dif z - \left[ \int_{-\infty}^{\infty} \tk((y - z)/h)\, \varrho(z) \, \dif z \right]^2 \\
&= h \int_{-\infty}^{\infty} [\tk(w)]^2\, \varrho(y + wh) \, \dif w - \left[ h \int_{-\infty}^{\infty} \tk(w)\, \varrho(y + wh) \, \dif w \right]^2 \\
& \qquad \qquad \text{where }w = (z - y)/h, \ z = y + wh
\end{align*}
%Luke Bielawski - the differential in the last integral should be dw, not dz. I have enacted this change.

\end{Problem}

\subsection{Bounding the Discrepancy}
We already know that the discrepancy is $\Order(n^{-1+\delta})$ for all $\delta > 0$.

\begin{Problem} [Later]
    After we solve the previous problem, we need to see how to optimally choose the coordinate weights with respect to $h$ to keep this small.
\end{Problem}



\section{Hermite Function Density Kernels}

Let $H_m$ denote the (``physicist's) Hermite polynomials, written as
\begin{equation*}
	H_m(y) = (-1)^m \exp(y^2) \, \frac{\dif^m \exp(-y^2)}{\dif y^m}, \; m \in \natzero, \qquad
	H_0(y) = 1, \ H_2(y) = 4y^2-2, \ H_4(y) = 16 y^4 - 48 y^2 + 12, \ \ldots.
\end{equation*}
These polynomials are orthogonal with respect to the weight $y \mapsto \exp(-y^2)$, i.e.,
\begin{equation*}
	\int_{-\infty}^{\infty} H_m(y) H_{m'}(y) \exp(-y^2) = \sqrt{\pi} 2^m m! \delta_{m,m'}, \quad m,m' \in \natzero.
\end{equation*}
Taking $\tk$ to be a linear combination of the first $r$ even Hermite polynomials multiplied by the weight function and enforcing \eqref{eq:tkprop} yields
\begin{align*}
	r& = 0  & \tk(y) &= \frac{\exp(-y^2)}{\sqrt{\pi}}
		\\
	r& = 1 &  \tk(y) & =\frac{\left(3 - 2y^{2}\right) \exp(- y^{2})}{2\sqrt{\pi}} \\
	r &= 2 & \tk(y) & = \frac{\left(4 y^{4} - 20 y^{2} + 15\right) \exp(- y^{2})}{8 \sqrt{\pi}}
\end{align*}

\bibliographystyle{amsplain}
\bibliography{FJH23,FJHown23}

\appendix

\section{The Reproducing Kernel Hilbert Space Trick} \label{app:rkhs}
A Hilbert space, $\ch$, is a vector space with an inner product, $\ip{\cdot}{\cdot}$ that is complete under the norm induced by the inner product.  A reproducing kernel Hilbert space \cite{RKHSwiki} is a Hilbert space of functions on a domain $\Omega$ with a kernel, $K:\Omega \times \Omega \to \reals$, such that function evaluation is a bounded linear functional, and
\begin{equation} \label{eq:repproperty}
    f(y) = \ip{K(\cdot,y)}{f} \qquad \forall y \in \Omega.
\end{equation}
Note that as a consequence of assuming $K(\cdot,y) \in \ch$ for all $y\in \ch$, it follows that
\begin{equation} \label{eq:rkip}
    \ip{K(\cdot,y)}{K(\cdot, z)} = K(y,z) \qquad \forall y,z \in \Omega.
\end{equation}

The Riesz representation theorem \cite{RieszWiki} states that any bounded/continuous linear functional $L:\ch \to \reals$ can be expressed as an inner product
\begin{equation}\label{eq:RTT}
L(f) = \ip{g}{f} \qquad \forall f \in \ch,
\end{equation}
where the representer, $g \in \ch$, is uniquely determined by $L$.  This is true for all Hilbert spaces, but the theorem does not tell one how to construct the representer.

Combining \eqref{eq:repproperty} and \eqref{eq:RTT} provides a formula for the representer in terms of the linear functional, $L$, applied to the reproducing kernel, $K$.  Note that
\begin{equation} \label{eq:repformula}
    g(y) \underbrace{=}_{\text{by \eqref{eq:repproperty}}} \ip{K(\cdot, y)}{g}
    \underbrace{=}_{\text{by \eqref{eq:RTT}}} L(K(\cdot,y)) .
\end{equation}

Now we illustrate how to use \eqref{eq:repformula} to derive \eqref{eq:firstbd}.  Note that $L: \varrho \mapsto \varrho(y) - \int_{-\infty}^{\infty} k(z,y) \, \varrho(z) \, \dif z$ is a bounded linear functional.  Let $g$ denote its representer.  Then by the Cauchy-Scwhartz inequality \cite{CauchySchwarzWiki}, it follows that
\begin{equation}
    \abs{\varrho(y) - \int_{-\infty}^{\infty} k(z,y) \, \varrho(z) \, \dif z} = \abs{L(\varrho)} = \abs{\ip{g}{\varrho}} \le \norm{g}\norm{\varrho}
\end{equation}
This looks like  \eqref{eq:firstbd}, except that we need the formula for $\norm{g}$ as in \eqref{eq:kquality}.

This can be obtained via \eqref{eq:repformula}.  Since we assume that $\varrho$ is in the Hilbert space defined by the reproducing kernel $\KY$
\begin{align*}
    g(w) &= L(\KY(\cdot,w)) \qquad \forall w \in \reals \\
    & = \KY(y,w) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,w) \, \dif z \qquad \forall w \in \reals, \\
    \norm{g}^2 &= \ip{g}{g} \\
    & = \ip{\KY(y\cdot) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z}{\KY(y,\cdot) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z} \\
    & = \ip{\KY(y,\cdot)}{\KY(y,\cdot)}
      - 2 \ip{\KY(y,\cdot)}{\int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z} \\
     & \qquad \qquad   + \ip{\int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z}{\int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z} \\
    & = \KY(y,y)
      - 2 \int_{-\infty}^{\infty} k(z,y) \, \ip{\KY(y,\cdot)}{\KY(z,\cdot)} \, \dif z \\
    & \qquad \qquad   + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} k(z,y) k(w,y) \, \ip{ \KY(z,\cdot)}{\KY(w,\cdot)} \,  \dif z\dif w \\
   & = \KY(y,y)
      - 2 \int_{-\infty}^{\infty} k(z,y) \, \KY(y,z) \, \dif z
     + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} k(z,y) k(w,y) \, \KY(z,w) \,  \dif z\dif w.
\end{align*}
%Luke Bielawski - I believe there is a typo in the last double integral. The last $\KY(w, \cdot)$$ shouldn't be there as $\ip{ \KY(z,\cdot)}{\KY(w,\cdot)}$ should just be $\KY(z, w)$ with no extra term.
%I have made this change
This completes the derivation of \eqref{eq:kquality}.

\section{Mean Squared Error Under IID Sampling} \label{app:MSE}
Let $Y$ be a random variable with mean $\mu :=  \Ex(Y)$ and variance $\var(Y) = \Ex[(Y - \mu)^2] = \Ex(Y^2) - \mu^2$.  Let $Y_1, Y_2, \ldots$ be independent and identically distributed (IID) instances of $Y$, and denote the sample mean by $\hmu_n = n^{-1} \sum_{i=1}^n Y_i$.  Note that $\Ex(\hmu_n) = \mu$.  Then the mean square error in approximating the population (true) mean by the sample mean is
\begin{equation*}
    \Ex[(\mu - \hmu_n)]^2  =  \var(\hmu_n) = \var \left (\frac{1}{n} \sum_{i=1}^n Y_i \right) \underbrace{=}_{\text{since the $Y_i$ are IID}} \frac 1{n^2} \sum_{i=1}^n \var(Y_i) = \frac{\var(Y)}{n}
\end{equation*}

If $Y = g(\vX)$ for $\vX \sim \cu[0,1]^d$, then
\begin{gather*}
    \mu = \Ex(Y) = \Ex[g(\vX)]  = \int_{[0,1]^d} g(\vx) \, \dif \vx, \\
    \var(Y) = \var\bigl(g(\vX)\bigr) = \Ex\{[g(\vX)]^2\} - \{\Ex[g(\vX)]\}^2 = \int_{[0,1]^d} [g(\vx)]^2  \, \dif \vx -
    \left[\int_{[0,1]^d} g(\vx) \, \dif \vx\right]^2.
\end{gather*}
Thus, the mean squared error is
\[
\frac 1n  \left\{\int_{[0,1]^d} [g(\vx)]^2  \, \dif \vx -
    \left[\int_{[0,1]^d} g(\vx) \, \dif \vx\right]^2 \right\}.
\]

\section{Decompositions of Functions and Their Hilbert Spaces} \label{sec:decomp}
In cubature error analysis it often is helpful to decompose the Hilbert space and the functions in them into pieces that only depend on a subset of the coordianates, indexed by $\fu \subseteq \{1, \ldots, d\}$.  For example, any function $g:[0,1]^d \to \reals$ can be decomposed as follows recursively:
\begin{align*}
	&& g(\vx) & = \sum_{\fu \subseteq \{1, \ldots, d\}} g_\fu(\vx_\fu), \\
	&\text{where}& g_\emptyset &= g(1/2, \ldots, 1/2), \\
	&&g_{\fu}(\vx_\fu) &= g(\vx) \Bigr |_{\vx_{\overline{\fu}} = (1/2, \ldots, 1/2)} - \sum_{\fv \subset \fu} g_{\fv}(\vx_\fv).
\end{align*}

For example, if $g(\vx) = x_1 -2 \cos(x_2 x_3)$, then
\begin{align*}
	g_\emptyset &= g(1/2, \ldots, 1/2) = 1/2 - 2 \cos(1/4), \\
	g_{\{1\}}(x_1) &= g(x_1,1/2,1/2) - g_{\emptyset} = x_1 - 2 \cos(1/4) - ( 1/2 - 2 \cos(1/4)) = x_1 - 1/2,\\
	g_{\{2\}}(x_2) &= g(1/2,x_2,1/2) - g_\emptyset = 1/2 - 2 \cos(x_2/2) - (1/2 - 2 \cos(1/4)) \\
	& = -2 \cos(x_2/2) + 2\cos(1/4),\\
	g_{\{3\}}(x_3) &= g(1/2,1/2, x_3) - g_\emptyset = 1/2 - 2 \cos(x_3/2) - (1/2 - 2 \cos(1/4)) \\
	&= -2  \cos(x_3/2) + 2\cos(1/4),\\
	g_{\{1,2\}}(x_1,x_2) &= g(x_1,x_2,1/2) - g_\emptyset -g_{\{1\}}(x_1) -g_{\{2\}}(x_2), \\
	& = x_1 - 2 \cos(x_2/2) - (1/2 - 2 \cos(1/4)) - (x_1 - 1/2) - (-2\cos(x_2/2) +2 \cos(1/4)),\\
	& = 0,\\
	g_{\{1,3\}}(x_1,x_3) &= g(x_1,1/2,x_3) - g_\emptyset -g_{\{1\}}(x_1) -g_{\{3\}}(x_3), \\
	& = x_1 - 2 \cos(x_3/2) - (1/2 - 2 \cos(1/4)) - (x_1 - 1/2) - (-2\cos(x_3/2) +2 \cos(1/4)),\\
	& = 0, \\
	g_{\{2,3\}}(x_2,x_3) &= g(1/2,x_2,x_3) - g_\emptyset -g_{\{2\}}(x_2) -g_{\{3\}}(x_3), \\
	& = 1/2- 2 \cos(x_2x_3) - (1/2 - 2 \cos(1/4)) \\
	& \qquad \qquad - (-2\cos(x_2/2) +2 \cos(1/4)) - (-2\cos(x_3/2) +2 \cos(1/4)),\\
	& = -2\cos(x_2x_3) + 2 \cos(x_2/2) + 2 \cos(x_3/2) -2\cos(1/4), \\
	g_{\{1,2,3\}}(x_1,x_2,x_3) &= g(x_1,x_2,x_3) -
	g_\emptyset - g_{\{1\}}(x-1) - g_{\{2\}}(x_2) -g_{\{3\}}(x_3) \\
	& \qquad \qquad - g_{\{1,2\}}(x_1,x_2) - g_{\{1,3\}}(x_1,x_3) - g_{\{2,3\}}(x_2,x_3) \\
	& = x_1 -2 \cos(x_2 x_3) - (1/2 - 2 \cos(1/4)) - (x_1 - 1/2) - (-2\cos(x_2/2) +2 \cos(1/4)) \\
	& \qquad \qquad - (-2\cos(x_3/2) +2 \cos(1/4)) - 0 - 0 \\
	& \qquad \qquad - ( -2\cos(x_2x_3) + 2 \cos(x_2/2) + 2 \cos(x_3/2) -2\cos(1/4)) \\
	& = 0.
\end{align*}
From this decomposition we can conclude that $g$ has a constant term, pieces that depend $x_1$, $x_2$, and $x_3$ separately, and a piece that depends on $x_2$ and $x_3$ together.

A property of this decomposition is that  $g_\fu(\vx_u) \bigr \vert_{x_\ell = 1/2} = 0$ for all $\ell \in \fu$.

Now, decompose the reproducing kernel as follows:
\begin{align*}
	K_{\vx,\fu} (\vt_\fu,\vx_\fu) &:= \frac{1}{2^{\abs{\fu}}}\prod_{\ell \in \fu} [\abs{t_\ell -1/2} + \abs{x_\ell-1/2} - \abs{t_\ell  - x_\ell}] \\
	\KX(\vx,\vt) & = \sum_{\fu \subseteq \{1, \ldots, d\}} \gamma_\fu^2   K_{\vx,\fu} (\vt_\fu,\vx_\fu).
\end{align*}
The reproducing kernel $K_{\vx,\fu}$ corresponds to a Hilbert space of functions defined on $[0,1]^{\fu}$ that vanish if any $x_\ell = 1/2$ for $\ell \in \fu$.  Moreover,
\begin{equation*}
	\norm[\KXu]{g_\fu}^2 = \int_{[0,1]^{\fu}} \abs{\frac{\partial^{\abs{\fu}} g_{u}}{\partial \vx_u}}^2 \, \dif \vx_\fu =  \int_{[0,1]^{\fu}} \abs{\left . \frac{\partial^{\abs{\fu}} g_\fu}{\partial \vx_u} \right \rvert_{\vx_{\overline{\fu}} = (1/2, \ldots, 1/2)}}^2 \, \dif \vx_\fu.
\end{equation*}

Then, it follows that the cubature error of $g$ in the Hilbert space with reproducing kernel $\KX$ can be bounded as
\begin{align*}
	%\label{eq:secondbddecomp}
	\abs{\int_{[0,1]^d} g(\vx) \,  \dif (F_{\mathrm{unif},\fu} - F_{\mX_\fu})(\vx)}^2
	& = \abs{\int_{[0,1]^d} \sum_{\fu \subseteq \{1, \ldots, d} g_{\fu}(\vx_\fu) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)}^2\\
	& = \abs{\sum_{\fu \subseteq \{1, \ldots, d} \int_{[0,1]^\fu} g_{\fu}(\vx_\fu) \,  \dif (F_{\mathrm{unif},\fu} - F_{\mX_\fu})(\vx)}^2\\
	& = \abs{\sum_{\fu \subseteq \{1, \ldots, d} \ip[K_{\vx,\fu}]{\int_{[0,1]^{\fu}} K_{\vx,\fu}(\cdot,\vx_\fu) \,  \dif (F_{\mathrm{unif},\fu} - F_{\mX_\fu})(\vx)}{g_u}}^2 \\
	\nonumber
	& \le  \sum_{\emptyset \ne \fu \subseteq \{1, \ldots, d\}} \norm[\KXu]{\int_{[0,1]^\fu} K_{\vx,\fu}(\cdot,\vx_\fu) \,  \dif (F_{\mathrm{unif},\fu} - F_{\mX_\fu})(\vx)}^2\, \norm[K_{\vx,\vu}]{g_\fu }^2,
\end{align*}
where $F_{\mathrm{unif},\fu}$ is the uniform distribution over $[0,1]^\fu$, and $\mX_\fu$ is the submatrix of $\mX$ composed of the columns indexed by $\fu$.
Here, (see \cite{Hic99b})
\begin{align*}
	\MoveEqLeft{\norm[\KXu]{\int_{[0,1]^\fu} K_{\vx,\fu}(\cdot,\vx_\fu) \,  \dif (F_{\mathrm{unif}} - F_{\mX})(\vx)}^2} \\
	 & =
	\int_{[0,1]^{\fu} \times [0,1]^{\fu}}  K_{\vx,\fu}(\vt_\fu,\vx_\fu) \,  \dif \vt_\fu \dif \vx_\fu
	- \frac 2n \sum_{i=1}^n \int_{[0,1]^{\fu}} K_{\vx,\fu}(\vt_\fu,\vx_{i,\fu}) \,  \dif \vt_\fu
	+ \frac 1 {n^2} \sum_{i,j=1}^n K_{\vx,\fu}(\vt_{i,\fu},\vx_{j,\fu}) \\
	& = \frac{1}{12^{\abs{\fu}}} - \frac {2}{2^{\abs{\fu}}} \sum_{i=1}^n \prod_{\ell \in \fu} \left(\abs{x_{i\ell} - 1/2} - \abs{x_{i\ell} - 1/2}^2 \right) \\
	& \qquad \qquad + \frac 1 {2^{\abs{\fu}}n^2} \sum_{i,j=1}^n
	\prod_{\ell \in \fu} [\abs{x_{i,\ell} -1/2} + \abs{x_{j,\ell}-1/2} - \abs{x_{j,\ell}  - x_{j,\ell}}]
\end{align*}


Now what does $\norm[\KXu]{g_\fu}^2$ look like when $g(\vx) = \tk\bigl((y-f(\vx))/h\bigr)/h = \tk\bigl((f(\vx) - y)/h\bigr)/h$?
\begin{align*}
	\frac{\partial g}{\partial x_\ell} &= \frac 1{h} \left[ \frac 1h \tk^{(1)}\left(\frac{f(\vx) - y}{h} \right) \frac{\partial f}{\partial x_\ell} \right] \\
	\frac{\partial^2 g}{\partial x_{\ell_1} \partial x_{\ell_2}}
	&= \frac 1{h} \left[\frac 1{h^2} \tk^{(2)}\left(\frac{f(\vx) - y}{h} \right)  \frac{\partial f}{\partial x_{\ell_1}}  \frac{\partial f}{\partial x_{\ell_2}} + \frac 1h \tk^{(1)}\left(\frac{f(\vx) - y}{h} \right)  \frac{\partial^2 f}{\partial x_{\ell_1}\partial x_{\ell_2}} \right] \\
	\frac{\partial^3 g}{\partial x_{\ell_1} \partial x_{\ell_2} \partial x_{\ell_3}}
&= \frac 1{h} \left[\frac 1{h^3} \tk^{(3)}\left(\frac{f(\vx) - y}{h} \right)  \frac{\partial f}{\partial x_{\ell_1}}  \frac{\partial f}{\partial x_{\ell_2}} \frac{\partial f}{\partial x_{\ell_3}} \\
& \qquad + \frac 1{h^2} \tk^{(2)}\left(\frac{f(\vx) - y}{h} \right)
\left \{ \frac{\partial^2 f}{\partial x_{\ell_1}\partial x_{\ell_2}} \frac{\partial f}{\partial x_{\ell_3}}
+ \frac{\partial^2 f}{\partial x_{\ell_1}\partial x_{\ell_3}} \frac{\partial f}{\partial x_{\ell_2}}
+ \frac{\partial^2 f}{\partial x_{\ell_2}\partial x_{\ell_3}} \frac{\partial f}{\partial x_{\ell_1}}
\right\} \\
& \qquad + \frac 1{h} \tk^{(2)}\left(\frac{f(\vx) - y}{h} \right)
\frac{\partial^3 f}{\partial x_{\ell_1} \partial x_{\ell_2} \partial x_{\ell_3}}
 \right ] \\
	\frac {\partial g}{\partial \vx_\fu} &=
	\frac 1h \sum_{m=1}^{\abs{\fu}} \left \{ \frac 1 {h^m} \tk^{(\abs{\fu}-m+1)}\left(\frac{f(\vx) - y}{h} \right) \sum_{} ???? \right\}
\end{align*}





\end{document}

\section{Error in Estimating the Smoothed Density with Tensor Product Kernels}\label{sec:errsmthdens}
Now we look at the second part of the error namely, $	\abs{\hvarrho(y,\varrho,k) - \hvarrho(y,\varrho_{\vy},k)}$.  We also assume that $\KX$ is of tensor product form, namely,
\begin{equation*}
	\KX(\vt,\vx) = \prod_{\ell = 1}^d [1 + \gamma_\ell \hKX(t_\ell,x_\ell) ] = \sum_{\fu \subseteq \{1, \ldots, d\}} \gamma_{\fu} \KX_{\fu}(\vt_\fu,\vx_\fu),
\end{equation*}
where the $\gamma_\ell$ are coordinate weights, $\gamma_\fu = \prod_{\ell \in \fu} \gamma_\ell}$, and $\KX_{\fu}(\vt_\fu,\vx_\fu) = \prod_{\ell \in \fu} \hKX(t_\ell,x_\ell)$.
Moreover, $\vx_\fu$ is the vector of components of $\vx$ indexed by the set $\fu$. Any $g$ in the Hilbert space with reproducing kernel $\KX$, one may be decomposed as $g(\vx) = \sum_{\fu \subseteq \{1, \ldots, d\}} g_\fu(\vx_\fu)$, where each $g_\fu$ is in the Hilbert space with reproducing kernel $\hKX_\fu$.

Under these assumptions, the error in \eqref{eq"??} can be bounded as
\begin{equation*}
	\abs{\hvarrho(y,\varrho,k) - \hvarrho(y,\varrho_{\vy},k)} \le  \norm[\KX]{\int_{[0,1]^d} \KX(\cdot,\vx) \, \dif \vx -
		\frac 1n \sum_{i=1}^n \KX(\cdot,\vx_i)} \norm[\KX]{k(f(\cdot),y) } ,
\end{equation*}
where
\begin{align*}
	\MoveEqLeft{\norm[\KX]{\int_{[0,1]^d} \KX(\cdot,\vx) \, \dif \vx -
			\frac 1n \sum_{i=1}^n \KX(\cdot,\vx_i)}^2} \\
	&=  \sum_{\fu \subseteq \{1, \ldots, d\}} \gamma_\fu \bigg [\int_{[0,1]^\fu \times [0,1]^\fu} \KX_\fu(\vx_\fu,\vt_\fu) \, \dif \vx_\fu \, \dif \vt_\fu -
	\frac 2n \sum_{i=1}^n \int_{[0,1]^\fu} \KX(\vx_\fu,\vx_{i,\fu}) \, \dif \vx_\fu \\
	& \qquad \qquad + \frac 1{n^2} \sum_{i,j=1}^n  \KX(\vx_{i,\fu},\vx_{j,\fu}) \right \bigg].
\end{align*}
This is an upper bound on the second term in the expression for the error that separates the part depending on the choice of the density kernel evaluated at $(f(\cdot),y)$ from the part depending on the sample nodes.










\bibliographystyle{alphadin}
\bibliography{FJH23,FJHown23}

\end{document}



\section{Stationary Density Kernels}

Suppose that $k(z,y) = h^{-1}\tk((z-y)/h)$ where $h$ is a bandwidth, and $\KY(y,z) = \tKY(y-z)$.  Then the quantity measuring the quality of the density kernel in \eqref{eq:kquality} may be written as
\begin{align*}
	\MoveEqLeft{\norm[\KY]{\KY(\cdot,y) - \int_{-\infty}^{\infty} k(z,y) \, \KY(z,\cdot) \, \dif z}^2} \\
	&=  \tKY(0) -  \frac 2h \int_{-\infty}^{\infty} \tk((z-y)/h) \, \tKY(z-y) \, \dif z \\
	& \qquad \qquad + \frac 1{h^2} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \tk((z-y)/h)  \, \tKY(z-t) \, \tk((t-y)/h) \, \dif z \, \dif t \\
	&=   \tKY(0) - \frac{2}h \int_{-\infty}^{\infty} \tk(w/h) \, \tKY(w) \, \dif w \\
	& \qquad \qquad + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \tk(w) \, \tKY(h(w-v)) \, \tk(v)\, \dif w \, \dif v
	.
\end{align*}
via the variable transformations $w = (z-y)$ and $v = (t-y)$.

Let's approximate $\tKY$ by a MacLaurin series with $r+1$ terms.  Since $\tKY$ is an even function, the odd numbered derivatives drop out.
\begin{equation*}
	\tKY(y)  = \sum_{\ell = 0}^{r} \frac{\tKY^{(2\ell)}(0) \, y^{2\ell}}{2\ell!} + \int_0^y \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!} \, \dif z.
\end{equation*}
Here $\tKY^{(2\ell)}$ denotes the $2\ell^{\text{th}}$ derivative of $\tKY$.

It follows from this approximation that
\begin{align} \label{eq:isomidterm}
	\frac 1h \int_{-\infty}^{\infty} \tk(w/h) \, \tKY(w) \, \dif w & =
	\frac 1h \int_{-\infty}^{\infty} \tk(w/h) \, \sum_{\ell = 0}^{r} \frac{\tKY^{(2\ell)}(0) \, w^{2\ell}}{2\ell!}  \, \dif w \\
	&
	\nonumber
	\qquad \qquad
	+ \frac 1h  \int_{-\infty}^{\infty} \tk(w/h) \,  \int_0^{w} \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!} \, \dif z  \, \dif w
\end{align}
If $\tk$ has the property that
\begin{equation*}
	\int_{-\infty}^{\infty} \tk(w) w^{2\ell}  \, \dif w =  \delta_{0,\ell},
\end{equation*}
then the first term on the right side of \eqref{eq:isomidterm} reduces to $\tKY(0)$.  By reordering the iterated integral, the  second term on the right side of  \eqref{eq:isomidterm} can be written as
\begin{align*}
	\int_{0}^{\infty}  \frac 1h  \int_{z}^\infty \tk(w/h) \, \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!}   \, \dif w \, \dif z
	& =  \int_{0}^{\infty}  \ttk(z/h) \, \frac{\tKY^{(2r+2)}(z) \,  z^{2r+2}}{(2r+2)!}    \, \dif z
\end{align*}



